# 弱监督 Query–Chunk 管线配置

data:
  qa_mapping: "finglm_data_store/type1_1co1yr_pdf_map.csv"
  answers: "finglm_data_store/finglm_master.jsonl"
  pdf_dir: "data/input/finglm_report_pdf"
  chunk_output: "data/output/chunks"
  retrieval_output: "data/output/retrieval"
  annotations_output: "data/output/annotations"
  pair_output: "data/output/weak_pairs"
  summary: "finglm_data_store/type1_1co1yr_pdf_summary.csv"

zenparse:
  config: "config/zenparse_config.yaml"  # ZenParse 解析与分块参数

stages:
  stage0:
    pdf_count_cap: 20   # 安全上限，防止长尾 PDF 无限累加
    qa_count: 60        # 以 QA 数为主目标，优先多 QA PDF 直到接近该值
    description: "内部调试，优先选多QA的PDF"
  stage1:
    pdf_count_cap: 40
    qa_count: 100
  stage2:
    pdf_count_cap: 70
    qa_count: 300
  stage3:
    pdf_count_cap: 130
    qa_count: 800

retrieval:
  embedding_model: "Qwen/Qwen3-Embedding-0.6B"  # 可切换为 "bge-m3"
  model_backend: "sentence-transformers"
  top_k: 30
  batch_size: 16
  max_seq_length: 512         # 限制序列长度，降低显存占用
  empty_cache_interval: 5     # 每N个batch清理一次缓存，决定是否周期性调用 gc.collect() 及 mps/cuda empty_cache()，降低峰值。
  precision_mode: "auto"      # auto | fp16 | fp32；auto 非 CPU 默认尝试半精度以省显存。
  enable_empty_cache: true    # 控制是否周期性 empty_cache
  use_inference_mode: true    # 控制是否使用 torch.inference_mode()，减少无谓梯度占用。
  mem_log_interval: 10      # 内存日志频率，null=不打印；填整数（如10）表示每N个batch打印一次显存日志
  normalize_embeddings: true
  device: "mps" # or "mps" or "cuda"
  restrict_to_query_pdf: true      # 仅在与 query 同 pdf_stem 的 chunks 内检索
  section_blacklist_enabled: false # 默认关闭，打开后按下方黑名单过滤噪声段
  section_blacklist:
    - "释义"
    - "董事"
    - "监事"
    - "高级管理人员"
    - "员工"
    - "社会责任"
    - "其他重要事项"
  chunk_level: "child"         # 以子块为检索主键
  parent_concat: false         # embedding 不拼接 parent；parent 仅供精筛/生成时取上下文

  # 3a-0: Gold 标注与阈值校准
  gold_annotation:
    enabled: true                                # Stage0 必须先标注
    path: "data/output/annotations/gold_stage0.jsonl"
    sample_size: 20                              # 每个 stage 标注 20-30 条

  thresholds:
    source: "gold"                               # "gold" | "distribution" | "manual"
    positive_threshold: null                     # 从 gold 自动计算
    negative_threshold: null                     # 从 gold 自动计算

  # 3a-1: BM25
  bm25:
    enabled: false                               # Stage0 可选，Stage1+ 建议开启
    top_k: 50

  # 3a-2: Hybrid
  hybrid:
    enabled: false                               # 3a-1 完成后开启
    embedding_weight: 0.7
    bm25_weight: 0.3
    fusion_method: "rrf"                         # "rrf" | "weighted_sum" | "max"

teacher:
  model: "deepseek-chat"
  api_key: null
  temperature: 0.1
  enable: false

weak_supervision:
  pair_generation:
    require_calibrated_thresholds: true          # 强制依赖 3a-0 阈值

    negative_sampling:
      intra_doc:
        enabled: true
        ratio: 2                                 # 每个正例对应 2 个同文档难负例
      cross_doc:
        enabled: true
        ratio: 1
        strategy: "same_company_diff_year"       # 优先同公司跨年
        fallback_strategy: "same_industry_diff_company"
